# -*- coding: utf-8 -*-
"""models comparison.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H1FbRvm9jKSWRd8lUXRzmj4VFlPb7uke

# 0. Import data and libraries
"""

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder

from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, cohen_kappa_score

from sklearn import set_config
set_config(transform_output="pandas")

url = "https://drive.google.com/file/d/12SKP4COXVfbn3eOT-9MEH0QC6XwGppf7/view?usp=sharing"
path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]
churn_df = pd.read_csv(path)

# X and y creation
X = churn_df.drop(columns="CustomerID")
y = X.pop("Churn")

# data splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

train_data = X_train.copy()
train_data['Churn'] = y_train

"""# 1. Dummy Model"""

y_train.value_counts()

len(y_train)

# we will predict 4504 zeros, so no churn
dummy_train_pred = pd.Series(0, index=range(4504))

#how good is this prediction? 83% accuracy
dummy_accuracy = accuracy_score(y_true = y_train,
                                y_pred = dummy_train_pred
                                )

round(dummy_accuracy, 2)

"""# 2. Preprocess data"""

# select categorical and numerical column names
X_cat_columns = X.select_dtypes(exclude="number").columns
X_num_columns = X.select_dtypes(include="number").columns

# create numerical pipeline
numeric_pipe = StandardScaler()

 # create categorical pipeline, with the OneHotEncoder
categoric_pipe = make_pipeline(
    OneHotEncoder(sparse_output=False,
                  handle_unknown='ignore',
                  drop='if_binary'),
    StandardScaler()
)

preprocessor = make_column_transformer(
    (numeric_pipe, X_num_columns),
    (categoric_pipe, X_cat_columns)
)

"""# 3. Train Models"""

# Define models and their parameter grids
models = {
    "Decision Tree": {
        "pipeline": make_pipeline(preprocessor,
                              DecisionTreeClassifier(class_weight='balanced', random_state=123)),
        "param_grid": {
            "decisiontreeclassifier__max_depth": range(6, 20, 2),
            "decisiontreeclassifier__min_samples_leaf": range(3, 15, 2)
        }
    },
    "KNN": {
        "pipeline": make_pipeline(preprocessor, KNeighborsClassifier()),
        "param_grid": {
            "kneighborsclassifier__n_neighbors": range(3, 21, 2),  # Try odd values of K from 1 to 29
            "kneighborsclassifier__weights": ['uniform', 'distance'],  # Try both weighting methods
            "kneighborsclassifier__metric": ['euclidean', 'manhattan', 'minkowski']  # Different distance metrics
        }
    },
    "Logistic Regression": {
        "pipeline": make_pipeline(preprocessor, LogisticRegression(random_state=123)),
        "param_grid": {
            "logisticregression__C": [0.001, 0.01, 0.1, 1, 10],              #Controls the trade-off between model complexity and accuracy, lower C-simpler model, less overfitting
            "logisticregression__solver": ["liblinear", "lbfgs", "saga"],    #Determines the optimization algorithm
            "logisticregression__penalty": ["l1", "l2"],                     #whether to use L1 (Lasso) or L2 (Ridge) regularization
            "logisticregression__class_weight": [None, "balanced"]           #Useful when classes are imbalanced (e.g., churn cases are much rarer)
        }
    },
    "Random Forest": {
        "pipeline": make_pipeline(preprocessor, RandomForestClassifier(class_weight='balanced', random_state=123)),
        "param_grid": {
            "randomforestclassifier__n_estimators": range(30, 140, 10),
            "randomforestclassifier__min_samples_split": range(2, 12, 2)
        }
    },
    "Support vector machine": {
        "pipeline": make_pipeline(preprocessor, SVC(probability=True, random_state=123)),
        "param_grid": {
            }
    },
    "MLP Classifier": {
        "pipeline": make_pipeline(preprocessor, MLPClassifier(random_state=123)),
        "param_grid": {
            'mlpclassifier__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],
            'mlpclassifier__activation': ['relu', 'tanh'],
            'mlpclassifier__solver': ['adam', 'lbfgs'],
            'mlpclassifier__learning_rate_init': [0.001, 0.01],
            'mlpclassifier__alpha': [0.0001, 0.01]
        }
    }
}

# Create a DataFrame to store training and test accuracy
assessment_df = pd.DataFrame(columns=['train', 'test'], index=models.keys()) # keys of the models dictionary are used to define the row labels (index) of the assessment_df
results = [] # list to store detailed evaluation metrics for model
trained_models = {} # dictonary to store models with GridSearchCV results

# Train and evaluate models
for model_name, model_info in models.items():
    print(f"Training {model_name}...")
    search = GridSearchCV(model_info["pipeline"], model_info["param_grid"], cv=10, verbose=1)
    search.fit(X_train, y_train)
    trained_models[model_name] = search  # saving each trained model (along with the results of the GridSearchCV) into the dictionary trained_models

    # Evaluate training and test accuracy
    train_accuracy = accuracy_score(y_true=y_train, y_pred=search.predict(X_train))
    test_accuracy = accuracy_score(y_true=y_test, y_pred=search.predict(X_test))
    assessment_df.loc[model_name, 'train'] = train_accuracy
    assessment_df.loc[model_name, 'test'] = test_accuracy

    # Compute additional metrics
    test_pred = search.predict(X_test)
    best_score = search.best_score_
    test_recall = recall_score(y_test, test_pred)
    test_precision = precision_score(y_test, test_pred)
    test_f1 = f1_score(y_test, test_pred)
    test_cohens_kappa = cohen_kappa_score(y_test, test_pred)

    # Store results
    results.append({
        "Model": model_name,
        "Best CV Score": best_score,
        "Test Accuracy": test_accuracy,
        "Test Recall": test_recall,
        "Test Precision": test_precision,
        "Test F1": test_f1,
        "Cohen's Kappa": test_cohens_kappa
    })

# transform to df
results_df = pd.DataFrame(results)

"""## Assess performance"""

# Print training and test accuracy comparison
print("\nTraining vs Test Accuracies:")
print(assessment_df)

# Print detailed model performance results
print("\nComparison of Model Performances:")
print(results_df)

# Identify the best model (here: MLP Classifier)
chosen_model = trained_models["MLP Classifier"]

# Print the best parameters
print("Best parameters for MLP Classifier:")
print(chosen_model.best_params_)